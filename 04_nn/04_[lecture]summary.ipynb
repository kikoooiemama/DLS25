{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjhOdg2mzZr0"
   },
   "source": [
    "* Лекция 1. История развития Deep Learning  **<a href=\"https://www.youtube.com/watch?v=ZfXpX8tMg-w&ab_channel=DeepLearningSchool\">YouTube</a>**  **<a href=\"https://vkvideo.ru/video-155161349_456239204?ref_domain=stepik.org\">VK</a>**\n",
    "* Лекция 2. Механизм обратного распространения ошибки **<a href=\"https://www.youtube.com/watch?v=-yiq1DRX9K0&ab_channel=DeepLearningSchool\">YouTube</a>**  **<a href=\"https://vkvideo.ru/video-155161349_456239205?ref_domain=stepik.org\">VK</a>**\n",
    "* Лекция 3. Полносвязная нейронная сеть **<a href=\"https://www.youtube.com/watch?v=O0nGKKFyYT4&ab_channel=DeepLearningSchool\">YouTube</a>**  **<a href=\"https://vkvideo.ru/video-155161349_456239206?ref_domain=stepik.org\">VK</a>**\n",
    "* Лекция 4. Функции активации **<a href=\"https://www.youtube.com/watch?v=3F7rydcAa0w&ab_channel=DeepLearningSchool\">YouTube</a>**  **<a href=\"https://vkvideo.ru/video-155161349_456239207?ref_domain=stepik.org\">VK</a>**\n",
    "* Семинар. Введение в библиотеку Pytorch **<a href=\"https://www.youtube.com/watch?v=EfgFeqc0H6M&ab_channel=DeepLearningSchool\">YouTube</a>**  **<a href=\"https://vkvideo.ru/video-155161349_456239208?ref_domain=stepik.org\">VK</a>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-h7yCBjzRAV"
   },
   "source": [
    "### 1. История\n",
    "- Нейроны - клетки нервной ткани.\n",
    "- 1943 - первая модель нейронной сети. McCulloch&Pitts (модель нейрона)\n",
    "- 1958 - перцептрон Розенблатта.\n",
    "- 1998 - LeNet-5 - не вызвал революцию.\n",
    "- 2012 - AlexNet - модель, похожая на LeNet. К 2012 накопились данные, которые раскрыли потенциал сверточных нейронных сетей. После этого начался бум активного развития нейронных сетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DwHdiXJOFVpN"
   },
   "source": [
    "### 2. Механизм BP\n",
    "- Если не получается разделить пространство объектов линейно, то можно использовать деревья. Или SVM с ядром, которое позволит перейти в другое пространство, которое уже будет линейно разделимым. Или создать новые признаки.\n",
    "- Нейронные сети автоматизируют ту работу, которая описана выше.\n",
    "- Нейронная сеть - несколько дифференцируемых моделей\n",
    "- Разные функции активации нужны, чтобы получить НЕЛИНЕЙНОЕ преобразование над нашими исходными признаками.\n",
    "- Гладкость функции не важна, главное уметь считать производную в точке.\n",
    "- Слой - линейное преобразование + (свертки) + нелинейная функция активации.\n",
    "- Входной слой - представление данных\n",
    "- Выходной слой - итоговое представление данных (число, вектор чисел, распределение вероятностей и т.д.)\n",
    "- Backpropogation - по сути вычисление производной сложной функции.\n",
    "\n",
    "Пример расчета BP для логистической регрессии:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dgy6_pQKGm2q"
   },
   "source": [
    "$$f(w, x) = \\frac{1}{1 + e^{-(w_0x_0 + w_1x_1 + w_2)}}$$\n",
    "$$\\frac{df}{dx} = \\frac{e^{-x}}{(1 + e^{-x})^2} = f(x) \\cdot(1 - f(x))$$\n",
    "\n",
    "\n",
    "$w_0 = 2 \\,\\,\\,\\,\\,\\,\\, → \\,\\,\\, w_0 \\cdot x_0 = -2 \\,\\,\\,\\,\\, ↓$<br>\n",
    "$x_0 = -1$<br>\n",
    "$w_1 = -3 \\,\\,\\, → \\,\\,\\, w_1 \\cdot x_1 = 6 \\,\\,\\,  → \\,∑ = 1 \\,\\, → \\,\\, \\cdot(-1) = -1 \\,\\, → \\,\\, e^{x} = 0.37 \\,\\, → \\,\\, +1 = 1.37 \\,\\, → \\,\\, \\frac{1}{x} = 0.73 $<br>\n",
    "$x_1 = -2$<br>\n",
    "$w_2 = -3 \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, →  \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, → \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, ↑$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZZsvFAwJTj1"
   },
   "source": [
    "$f(x) = \\frac{1}{x} \\,\\,\\,\\,\\,\\,\\,\\, → \\,\\,\\,\\,\\,\\,\\,\\, \\frac{df}{dx} = \\frac{-1}{x^2}$<br>\n",
    "$f(x) = c + x \\,\\,\\,\\,\\,\\,\\,\\, → \\,\\,\\,\\,\\,\\,\\,\\, \\frac{df}{dx} = 1$<br>\n",
    "$f(x) = e^x \\,\\,\\,\\,\\,\\,\\,\\, → \\,\\,\\,\\,\\,\\,\\,\\, \\frac{df}{dx} = e^x$<br>\n",
    "$f(x) = ax \\,\\,\\,\\,\\,\\,\\,\\, → \\,\\,\\,\\,\\,\\,\\,\\, \\frac{df}{dx} = a$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSUdsSC4JTnt"
   },
   "source": [
    "$dw_0 = -0.2 \\,\\,\\ ←  0.2$<br>\n",
    "$dx_0 = 0.4 \\,\\,\\,\\,\\,\\ ↩ \\,\\,\\,\\,\\, ↑$<br>\n",
    "$dw_1 = -0.4 \\,\\, ←  \\, 0.2 = (-1)\\cdot(-0.2) \\, ← \\, -0.2 = e^{-1} \\cdot (-0.53) \\, ← \\, -0.53 = 1 \\cdot (-0.53)  \\, ← \\, -0.53 = \\frac{-1}{1.37^2} \\cdot 1 \\, ← \\, 1 \\, ← \\, \\frac{df}{df}$<br>\n",
    "$dx_1 = -0.6 \\,\\, ↩ \\,\\,\\,\\,\\,\\, ↓$<br>\n",
    "$dw_2 = 0.2 ← \\,\\,\\,\\,\\,\\,\\,\\, ↩  $<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3oUtNymf-hJJ"
   },
   "source": [
    "В матричном виде:\n",
    "\n",
    "$y_1 = f_1(x)$\n",
    "\n",
    "$y_2 = f_1(x)$\n",
    "\n",
    "$.........$\n",
    "\n",
    "$y_n = f_n(x)$\n",
    "\n",
    "$x = \\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "..... \\\\\n",
    "x_m\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dX} =\n",
    "\\begin{bmatrix}\n",
    "∇f_1(x) \\\\\n",
    "∇f_2(x) \\\\\n",
    "..... \\\\\n",
    "∇f_n(x)\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "\\frac{d}{dx}f_1(x) \\\\\n",
    "\\frac{d}{dx}f_2(x) \\\\\n",
    "..... \\\\\n",
    "\\frac{d}{dx}f_n(x)\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "\\frac{d}{dx_1}f_1(x) & \\frac{d}{dx_2}f_1(x) & ... & \\frac{d}{dx_m}f_1(x)\\\\\n",
    "\\frac{d}{dx_1}f_2(x) & \\frac{d}{dx_2}f_2(x) & ... & \\frac{d}{dx_m}f_2(x)\\\\\n",
    "... & ... & ... & ... \\\\\n",
    "\\frac{d}{dx_1}f_n(x) & \\frac{d}{dx_2}f_n(x) & ... & \\frac{d}{dx_m}f_n(x)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "https://explained.ai/matrix-calculus/\n",
    "\n",
    "Мы посчитали направление, в котором нужно менять КАЖДЫЙ параметр (посчитали частные производные функции потерь по каждому параметру).\n",
    "Далее оптимизация по GD:\n",
    "\n",
    "$$x_{t+1} = x_{t} - learning\\_{rate} \\cdot dx$$\n",
    "\n",
    "\n",
    "Вот по такой схеме обучаются параметры на каждом слое."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQ8iA9ljCi0n"
   },
   "source": [
    "### 3. Полносвязная нейронная сеть (Dense, Fully-connected, FC)\n",
    "\n",
    "- Многослойный перцептрон (MLP, Multilayer Perceptron) - нейронная сеть, где все слои линейные и каждый нейрон слоя связан с каждым нейроном предыдущего слоя.\n",
    "- Параметры: $W$ - weights, $b$ - bias.\n",
    "- softmax: $\\frac{e^{y_i}}{\\sum_{i}^{n} e^{y_i}}$ - хорошо в классификации в поледнем слое.\n",
    "- $\\frac{dy}{dx} = \\begin{bmatrix}\n",
    "\\frac{dy_1}{dx_1} & \\frac{dy_1}{dx_2}\\\\\n",
    "\\frac{dy_2}{dx_1} & \\frac{dy_2}{dx_2}\\\\\n",
    "\\end{bmatrix}$;\n",
    "$\\frac{dz}{dx} = \\begin{bmatrix}\n",
    "\\frac{dy_1}{dx_1} & \\frac{dy_1}{dx_2}\\\\\n",
    "\\frac{dy_2}{dx_1} & \\frac{dy_2}{dx_2}\n",
    "\\end{bmatrix}^{T}  ⋅\n",
    "\\begin{bmatrix}\n",
    "\\frac{dz}{dy_1} \\\\\n",
    "\\frac{dz}{dy_2}\n",
    "\\end{bmatrix} = \\Big(\\frac{dy}{dx}\\Big)^{T} \\cdot \\frac{dz}{dy}$\n",
    "\n",
    "- Полносвязный слой - композиция линейного слоя и функции активаици\n",
    "- Вычисление градиента по весам, W:\n",
    "$$\\frac{dL}{dw_{ij}} =\\frac{dL}{dy_1} \\cdot \\frac{dy_1}{dw_{ij}} + \\frac{dL}{dy_2} \\cdot \\frac{dy_2}{dw_{ij}} = \\frac{dL}{dy_{i}} \\cdot x_j$$\n",
    "$$\\frac{dL}{dW} = \\begin{pmatrix}\n",
    "\\frac{dL}{dy_1} \\\\\n",
    "\\frac{dL}{dy_2}\n",
    "\\end{pmatrix} \\cdot\n",
    "(x_1, x_2) = \\frac{dL}{dy} \\cdot (x_1, x_2)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDkAD0Pz-q7B"
   },
   "source": [
    "### 4. Функции активации\n",
    "\n",
    "<!-- <img src='https://drive.google.com/uc?id=1AhwqEQAWD8M_U5fAqbiHpXgh4jGiLcLJ' width=\"800\"/> -->\n",
    "![activation_functions.png](img/activation_functions.png \"activation_functions\")\n",
    "\n",
    "#### 4.1 Sigmoid\n",
    "\n",
    "<!-- <img src='https://drive.google.com/uc?id=1ZJxu0xzDhe9IvIy8Okm4lPk4SO8hj6TI'/> -->\n",
    "![sigmoid.png](img/sigmoid.png \"sigmoid\")\n",
    "\n",
    "- хорошо интерпретируется [0; 1], дифференцируема.\n",
    "- значительные хвосты с производной равно 0 $⇒$ затухающие градиенты, так как все, что было перед сигмоидой зануляется.\n",
    "- сетки - линейные модели, они любят нормированные/центрированные данные, а сигмоида дает нецентрированный output (центрирование - среднее значение 0).\n",
    "- считать экспоненту очень затратно.\n",
    "\n",
    "#### 4.2 Tanh\n",
    "\n",
    "<!-- <img src='https://drive.google.com/uc?id=11Mt03gM6FXTqugH6Zs8DFsOcLe0a7wEa'/> -->\n",
    "![tanh.png](img/tanh.png \"tanh\")\n",
    "\n",
    "- хорошо интерпретируется [-1; 1], дифференцируема.\n",
    "- симметричный, центрированный output (среднее значение 0).\n",
    "- градиенты все также затухают как и в случае с сигмоидой.\n",
    "- ограниченный output нужен для RNN\n",
    "\n",
    "#### 4.3 ReLU\n",
    "\n",
    "<!-- <img src='https://drive.google.com/uc?id=1ElwM6TRA_n37MB8LQhsOZwRwiOQUTyn_'/> -->\n",
    "![relu.png](img/relu.png \"relu\")\n",
    "\n",
    "- производная либо 0, либо 1: нет затрат на вычисления. Для справки: ReLU считать быстрее тангсенса в 6 раз.\n",
    "- нецентрированный выход\n",
    "- слева на хвосте затухают граиденты. Производная 0.\n",
    "- чтоб сделать более центрированной, придумали Leaky $ReLU$. Центрирование + градиенты не так затухают - используется часто. Еще есть parametric ReLU.\n",
    "\n",
    "$$PReLU = max(αx, x)$$\n",
    "\n",
    "#### 4.4 ELU\n",
    "\n",
    "- все плюсы ReLU.\n",
    "- более гладкая.\n",
    "- не затухает.\n",
    "- считать дорого из-за экспоненты.\n",
    "\n",
    "Поэтому важно выбирать функцию активации в зависимости от того какую задачу решать. Правило большого пальца:\n",
    "- Используйте $ReLU$ как baseline подход, если нет ограничений\n",
    "- Попробуйте $Leaky ReLU$ или $ELU$\n",
    "- Если есть ограничение на выходную функцию, то используйте $tanh$\n",
    "- Сигмоиду используйте только, если вам нужно получить что-то вроде вероятности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNC+T3kzAZskfYfMhDx2dK4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
